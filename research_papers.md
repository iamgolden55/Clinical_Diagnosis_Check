Comparing Bio_ClinicalBERT vs. PubMedBERT for Clinical Information Extraction

Overview

Bio_ClinicalBERT (Emily Alsentzer et al., 2019) and PubMedBERT (MSR BiomedNLP, 2020) are both BERT-based language models tuned for biomedical domains, but they differ in their training data and intended use-cases. Bio_ClinicalBERT was trained on clinical texts (electronic health records), while PubMedBERT was trained on biomedical research literature. We compare them across several dimensions to determine which is better suited for extracting clinical information such as medications (with dosages), symptoms (and their attributes like severity/duration), diagnoses, and vital signs. Key factors include:
	•	Training Domain & Data Source
	•	Model Architecture & Tokenizer
	•	Performance on Clinical vs. Biomedical NER/Relation Tasks
	•	Benchmark Results & Real-World Usage
	•	Strengths & Weaknesses

Finally, we provide a recommendation for clinical information extraction tasks.

Training Data and Domain

Bio_ClinicalBERT is trained on clinical domain text. Specifically, it was initialized from BioBERT and further pre-trained on MIMIC-III hospital notes (an ICU EHR dataset) ￼ ￼. This means it has seen discharge summaries, progress notes, medication orders, lab results, and other real patient note sections. In total, about 880 million words of de-identified clinical notes from MIMIC-III were used ￼. Because of this, Bio_ClinicalBERT is intimately familiar with the language of clinical narratives – e.g. shorthand notations, acronyms (BP for blood pressure, HR for heart rate), telegraphic style, and formatting of vital signs or medication lists that are common in patient records.

PubMedBERT (specifically the microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract model) is trained on biomedical research text. It was pretrained from scratch on PubMed abstracts (over 3 billion words of text) ￼ ￼, and a separate variant was trained on full-text articles as well. Its vocabulary and training corpora are derived entirely from PubMed, which consists of formal scientific writing (journal abstracts and papers). This gives PubMedBERT strong knowledge of biomedical terminology (diseases, drug names, genes, etc.) and formal language, but not of clinical note jargon. For example, PubMedBERT has seen sentences like “Patients were administered 5 mg of warfarin orally for 7 days” in literature, but it may not have seen the shorthand style “Coumadin 5 mg PO qd x 1 week” often found in clinical notes.

Implications for our tasks: The domain mismatch is critical. Extracting medications, dosages, symptoms, etc., from clinical notes will likely favor Bio_ClinicalBERT, since its training data actually contains these kinds of mentions in context. PubMedBERT’s training data rarely includes patient-specific sequences of vitals or medication lists; it sees aggregated trial results or case study descriptions instead. In short, Bio_ClinicalBERT is in-domain for clinical narratives, whereas PubMedBERT is out-of-domain for them (but in-domain for research text). This often leads to substantial performance differences in downstream tasks, as we will see.

Architecture and Tokenizer

Both models share the base BERT architecture (110M parameters, 12 Transformer layers, 768 hidden size). The core difference lies not in the number of layers but in how they were initialized and the vocabulary they use:
	•	Bio_ClinicalBERT Architecture: It is a BERT-base model initialized from BioBERT (which itself was BERT-base further trained on biomedical text) ￼. Bio_ClinicalBERT uses the same WordPiece tokenizer as BioBERT/BERT. In fact, one critique is that it retains the original BERT vocabulary, which wasn’t specialized for clinical text ￼. (Alsentzer’s ClinicalBERT models were BERT-base cased or BioBERT-base cased models fine-tuned on clinical notes ￼.) This means common clinical terms or abbreviations might be broken into subwords if they weren’t in the original vocab. Nevertheless, through pretraining on MIMIC, the model learned representations for clinical jargon and context, even if the tokenizer is not domain-specific.
	•	PubMedBERT Architecture: PubMedBERT is also a BERT-base model, but it was trained from scratch instead of starting from general BERT ￼. Importantly, it uses a custom vocabulary built from PubMed data ￼ ￼. This domain-specific uncased vocabulary better covers biomedical terms (e.g. whole words like “naloxone” or “cardiomyocyte” that may be split or missing in general BERT vocab) ￼. PubMedBERT’s tokenizer thus handles biomedical terminology more gracefully, contributing to its strong performance on biomedical text. However, this vocab may lack tokens for clinical abbreviations or colloquial terms found in health records (for example, tokens like “qhs” or “BP” might not be explicitly present, or appear only in lowercased form).
	•	Cased vs Uncased: Bio_ClinicalBERT is a cased model (retaining letter case), whereas the PubMedBERT variant in question is uncased (all text lowercased). In clinical text, acronyms like “BP” (blood pressure) or drug brand names with capitalization could be distinct, but in practice this difference is minor for extraction tasks. The main impact is that PubMedBERT will treat “BP” and “bp” as the same token (likely “bp”), whereas Bio_ClinicalBERT might keep “BP” intact or as ‘B’ + ‘##P’ subwords if not in its vocab. Overall, casing is a small factor; the training domain matters more.

In summary, Bio_ClinicalBERT = BERT-base (cased) + BioBERT initialization + clinical notes data, and PubMedBERT = BERT-base (uncased) trained from scratch on PubMed with a new vocab. They are comparable model sizes, so differences in downstream performance will come from knowledge (corpus exposure) and tokenization coverage rather than model capacity.

Performance on Clinical NLP Tasks

To evaluate these models for clinical information extraction, we look at known results on tasks like clinical named entity recognition (NER) and relation extraction, especially those involving concepts such as problems, treatments, and measurements. Notably, the 2010 and 2012 i2b2 challenges are relevant benchmarks:
	•	i2b2 2010 focused on extracting medical concepts (Problems, Tests, Treatments) from clinical notes.
	•	i2b2 2012 involved entity extraction for temporal relations (clinical events and time expressions).
	•	i2b2 2009 (not explicitly cited in Alsentzer’s work) was about medication information extraction (medications, dosages, modes, frequencies).

Bio_ClinicalBERT has demonstrated strong performance on such clinical NER tasks. In the original paper, Alsentzer et al. showed that domain-specific BERT models outperform general BERT and BioBERT on clinical text. For instance, Bio+ClinicalBERT achieved about 87% F1 on the i2b2 2010 concept extraction task, significantly higher than non-clinical BERT models ￼. Likewise, it scored 78.9% F1 on the i2b2 2012 challenge (clinical event extraction) ￼. These were improvements of several points in F1 over a general BERT-base, confirming the value of in-domain pretraining. On a clinical natural language inference task (MedNLI), Bio_ClinicalBERT even set a new state-of-the-art with 82.7% accuracy ￼ ￼. In practical terms, this means Bio_ClinicalBERT is better at recognizing mentions of conditions, drugs, dosages, and symptoms in clinical text than a model without clinical training. Its understanding of context (e.g. that a phrase like “started on 5 mg prednisone daily” refers to a medication and dosage) is enhanced by having seen many such patterns in MIMIC notes.

By contrast, PubMedBERT has not been explicitly benchmarked on those clinical note tasks in published results, and its performance would likely lag without additional fine-tuning on clinical data. Microsoft’s researchers explicitly separated clinical vs. biomedical benchmarks when evaluating PubMedBERT ￼, because a model trained on one domain doesn’t excel in the other. In fact, a recent study noted that ClinicalBERT performed the worst on biomedical NER datasets, and suggested that models pre-trained on clinical notes are not well suited for BioNER (literature) tasks ￼. The inverse is also true: a model like PubMedBERT (pre-trained on literature) is not naturally adept at clinical text. We can expect PubMedBERT to miss or misidentify some entities in clinical notes unless fine-tuned on that type of text, because it hasn’t seen the idiosyncrasies of clinical narratives. For example, without exposure to clinical training data, PubMedBERT might struggle with a sentence like “Pt denies chest pain but reports intermittent palpitations x3 days” – identifying “chest pain” and “palpitations” as symptoms might be doable (since those words exist in literature), but linking “x3 days” to duration or understanding “Pt” as patient, might be less certain than Bio_ClinicalBERT which saw such phrasing frequently in MIMIC.

That said, if PubMedBERT is fine-tuned on a specific clinical NER dataset, it can certainly learn to perform the task. The question is starting point: Bio_ClinicalBERT gives a head-start with relevant representations. In one report, PubMedBERT fine-tuned on a clinical NER (like i2b2 2010) still did not match the accuracy of Bio_ClinicalBERT without domain adaptation ￼. In summary, for clinical entity extraction (medications, diagnoses, etc.), Bio_ClinicalBERT’s in-domain knowledge confers an advantage out of the box.

Relation extraction in the clinical domain (e.g. linking a medication to its dosage or a symptom to its severity) similarly benefits from in-domain understanding. Bio_ClinicalBERT, by virtue of capturing clinical context, may better infer relations like medication -> dosage or symptom -> duration, especially if fine-tuned on a relation dataset (e.g. i2b2 2010 also had a relation subtask for problem-treatment, etc.). PubMedBERT would again require exposure to clinical relational data to catch up. There aren’t widely reported head-to-head relation extraction results for these two on the same clinical dataset, but it follows the same pattern: domain alignment helps.

Performance on Biomedical Literature Tasks

For completeness, it’s worth noting how the models fare on biomedical literature information extraction, even though the focus here is clinical. On biomedical NER benchmarks derived from research articles (like the NCBI Disease corpus and BC5CDR chemical/disease corpus), PubMedBERT has excelled. According to the PubMedBERT paper and follow-up evaluations, PubMedBERT achieved state-of-the-art results on several biomedical NLP tasks in the BLURB benchmark ￼. For example, it attained the highest F1 on the BC5CDR disease and chemical entity recognition tasks ￼, outperforming BioBERT, ClinicalBERT, and others. On NCBI-disease, PubMedBERT was among the top performers (though SciBERT slightly edged it out) ￼. Overall, PubMedBERT “outperformed all models (BERT, RoBERTa, BioBERT, SciBERT, ClinicalBERT, BlueBERT) on BLURB with a score of 81.1” ￼. These results reinforce that PubMedBERT is the better choice for biomedical literature text—it recognizes formal biomedical entity names well and benefits from its domain-specific vocabulary (unlike ClinicalBERT, which fared poorly on those benchmarks due to vocabulary mismatch) ￼.

However, these strengths on literature do not directly translate to clinical note tasks. The text structure and vocabulary differences mean a model can be great on one and not on the other ￼ ￼. Indeed, ClinicalBERT’s weak performance on literature NER ￼ is a mirror image of what we’d expect for PubMedBERT on clinical NER without further training.

Bottom line: PubMedBERT is superior for extracting information from research articles, academic reports, or any text similar to PubMed data. Bio_ClinicalBERT is superior for extracting information from EHR notes, clinical narratives, or texts similar to MIMIC notes. Each model is tuned to its domain’s language and quirks.

Benchmark Scores and Evaluations

To summarize some benchmark comparisons relevant to our focus (clinical entities):
	•	i2b2 Clinical Challenges – Bio_ClinicalBERT showed strong results:
	•	i2b2 2010 (concept extraction): ~87% F1 with Bio+ClinicalBERT ￼, beating general BERT/BioBERT on identifying problems, tests, treatments in notes.
	•	i2b2 2012 (temporal/entity): ~78.9% F1 ￼ with Bio+ClinicalBERT, again an improvement over baselines on event/entity extraction.
	•	MedNLI (clinical inference): 82.7% accuracy ￼ (SOTA at the time), showing BioClinicalBERT’s strength in understanding clinical language nuance.
	•	i2b2 2006 (de-identification): Interestingly, Bio_ClinicalBERT did 94% F1, slightly underperforming BioBERT’s 94.8% ￼ on this specific task. This was attributed to differences in how identifying personal health information is represented in training vs. test data ￼, and doesn’t reflect the typical clinical concept extraction scenario.
	•	BLURB/BLUE Biomedical Benchmarks – PubMedBERT’s highlights:
	•	NCBI Disease NER: ~87.8% F1 (near state-of-art; SciBERT ~88% best) ￼ ￼.
	•	BC5CDR Disease NER: Best model (PubMedBERT > SciBERT > others) ￼.
	•	BC5CDR Chemical NER: Best model (PubMedBERT > others) ￼.
	•	ChemProt relation extraction: PubMedBERT also achieved SOTA or near-SOTA on this chemical-protein interaction RE task (as reported in BLURB results ￼).
	•	BLURB Score: 81.1 (aggregate) for PubMedBERT vs. lower scores for BioBERT, ClinicalBERT, etc. ￼.

These scores reinforce that each model shines on tasks that resemble its training data. Notably, ClinicalBERT was the worst performer on the literature-based NER tasks (NCBI/BC5) ￼, and conversely we would expect PubMedBERT to be a poor performer on the i2b2 tasks unless it undergoes additional training on clinical text.

Real-World Usage and Case Studies

Both models are actively used in the community, but for different applications:
	•	Bio_ClinicalBERT Usage: This model is a go-to for many clinical NLP projects. It has been downloaded millions of times ￼ and fine-tuned for various downstream tasks such as extracting medical problems from doctor’s notes, identifying medications in prescriptions, or classifying patient status from notes. For example, researchers have fine-tuned Bio_ClinicalBERT on radiology report text and on specific NER datasets (some Hugging Face community models are Bio_ClinicalBERT fine-tunes on tasks like medical condition identification) ￼ ￼. Its popularity stems from the immediate performance boost it provides on EHR data compared to using a generic model. In a COVID-19 clinical query system, Bio_ClinicalBERT was used to retrieve patient history from notes ￼. Overall, its strength in understanding clinical context makes it very suitable for real-world hospital data extraction pipelines.
	•	PubMedBERT Usage: This model is commonly used in text mining of biomedical literature – for example, extracting drug-disease associations from research papers, question-answering on academic articles, or classifying scientific abstracts. It’s part of the standard arsenal for biomedical text mining competitions and was especially popular during the COVID-19 literature surge, to analyze papers. Some users have fine-tuned PubMedBERT on tasks like biomedical NLI or NER (e.g., fine-tuning on the NCBI Disease corpus yields state-of-the-art results ￼). However, in the clinical context, PubMedBERT is less often the first choice unless the text is more narrative (like a case report in a journal). It’s possible to adapt PubMedBERT to EHR data by further pretraining or fine-tuning on clinical text, but out-of-the-box it’s geared to a different subdomain of biomedical NLP.

One interesting case study noted that using PubMedBERT for a clinical semantic textual similarity task did not improve results, because the model’s PubMed-centric vocabulary wasn’t well-matched to the clinical text input ￼. This underscores that applicability is domain-dependent: many practitioners choose Bio_ClinicalBERT (or similar clinical models like ClinicalBioBERT or BlueBERT) for healthcare text, and PubMedBERT for scientific text.

Strengths and Weaknesses Summary

Bio_ClinicalBERT:
	•	Strengths:
	•	In-Domain Clinical Knowledge – Vocabulary and style of EHR notes are baked into the model. It recognizes medical shorthand, common abbreviations, and note structure (e.g. it will more easily parse a list of vitals or a medication list in a discharge summary) ￼.
	•	Strong Clinical NER Performance – Demonstrated high F1 on clinical entity extraction tasks (e.g. 87% F1 on i2b2 2010 concepts) ￼. Particularly good at identifying problems, medications, symptoms in context that mirrors real patient notes.
	•	Contextual Understanding of Clinical Concepts – Because it was further trained on clinical text after BioBERT, it combines biomedical knowledge with clinical context. It often outperforms general biomedical models when the text is actual patient data ￼.
	•	Widely Fine-Tuned – Many downstream models and research have built on Bio_ClinicalBERT for various tasks, indicating it’s robust and adaptable for clinical NLP.
	•	Weaknesses:
	•	Out-of-Domain for Scientific Text – Performs poorly on tasks involving academic text or formal biomedical writing (e.g., it was the worst of biomedical BERT variants on PubMed-based NER benchmarks) ￼. The general BERT vocabulary it uses can limit its coverage of scientific terms not seen in MIMIC.
	•	May Miss Newer Terms – MIMIC-III data is somewhat dated (up to 2012) and specific to ICU patients in one hospital. Bio_ClinicalBERT might not have seen some newer drug names or less common conditions not present in that corpus, whereas PubMedBERT might, having a broader literature base.
	•	Cased Vocab Complexity – Minor inconvenience: since it’s cased, one must be mindful of matching the case in input (though this is usually trivial).

PubMedBERT:
	•	Strengths:
	•	Broad Biomedical Terminology Coverage – Custom vocabulary and training on 21GB of PubMed text give it excellent coverage of biomedical terms, chemical names, gene names, etc. It tends to know the proper words for diseases and drugs as used in publications (helpful for formal text extraction) ￼ ￼.
	•	State-of-the-Art on Biomedical NLP – Achieves top-tier results on multiple benchmarks like NER (diseases, chemicals) and relation extraction in the biomedical literature domain ￼. It’s very powerful for tasks like extracting drug–gene interactions or symptoms from case reports in journals.
	•	From-Scratch Training – Because it wasn’t initialized from general BERT, it doesn’t carry over any irrelevant biases from Wikipedia/news data. It is purely biomedical, which in its domain led to better performance than models that were fine-tuned from general BERT ￼.
	•	Larger Training Corpora – PubMed is huge and diverse. The model has likely seen descriptions of a vast array of conditions and treatments (though in formal language), which could be useful if your clinical text mentions an obscure disease; PubMedBERT might recognize the term even if BioClinicalBERT didn’t see it in MIMIC.
	•	Weaknesses:
	•	Not Familiar with Clinical Jargon – Lacks exposure to EHR-specific language. Abbreviations like “HPI” (history of present illness) or “NKDA” (no known drug allergies), or shorthand like “c/o” (complains of) are not part of its training. It may need additional training to handle these reliably.
	•	Struggles with Note Style – Clinical notes often omit subjects, use telegraphic phrases, and include structured sections (headers like “Medications:” followed by lists). PubMedBERT has only seen full sentences in academic prose. This stylistic gap can degrade its out-of-the-box accuracy for extraction (for example, distinguishing a list of vital sign values from narrative text).
	•	Uncased – Might Lose Info – While usually fine, uncasing means it doesn’t inherently distinguish e.g. “Augmentin” (a brand name) from “augmentin” if such distinction mattered. In clinical notes, some proper nouns (hospital names, etc.) might be upper-case tokens that get lowercased and split. This is a minor issue, often mitigated by context, but still a difference from Bio_ClinicalBERT.
	•	Less Proven on Clinical Data – There are fewer published use-cases of PubMedBERT on hospital note datasets. It’s a bit of an unknown how well it would do on, say, an i2b2 medication extraction task without further domain adaptation. The expectation is it would need extra fine-tuning to match BioClinicalBERT’s level on those tasks.

Recommendation

For the goal of extracting medications (and dosages), symptoms (with severity/duration), diagnoses, and vital signs from clinical texts, Bio_ClinicalBERT is the more appropriate choice. Its training on actual clinical notes gives it a domain-specific understanding that directly aligns with these information types. It has effectively seen the patterns of how medications and dosages are documented, how symptoms and their attributes are mentioned by clinicians, and how vital signs are recorded in patient charts. This translates to higher accuracy in recognizing and categorizing those entities in unstructured clinical text ￼.

PubMedBERT, while a powerful model in its own right, is better suited for biomedical literature mining than for raw clinical note mining. If your documents were research papers or medical case reports, PubMedBERT would excel at identifying medical terms and relationships. But in an EHR context, it would likely miss nuances without additional fine-tuning on clinical data. As one study pointed out, a model like ClinicalBERT (trained on notes) significantly outperforms PubMed-trained models on clinical entity extraction ￼ ￼. This indicates that to use PubMedBERT for the listed clinical NLP tasks, you’d have to invest in further training with clinical datasets, essentially trying to teach it what Bio_ClinicalBERT already knows.

In summary, Bio_ClinicalBERT’s strengths align perfectly with clinical information extraction: it was literally built for that purpose, and it has a proven track record on relevant benchmarks. PubMedBERT is an excellent model for biomedical text, but for the specific needs of medications, symptoms, diagnoses, and vitals in clinical narratives, it is not the optimal choice unless no clinical-specific model is available. Therefore, we recommend using Bio_ClinicalBERT for clinical NLP extraction tasks due to its domain-specific training and superior performance on clinical NER, and using PubMedBERT only if your text is more scientific than clinical or if you plan to further tailor PubMedBERT to the clinical domain with additional data.

References:
	•	Alsentzer et al. (2019), “Publicly Available Clinical BERT Embeddings” – introduced ClinicalBERT and showed its improvements on i2b2 tasks ￼ ￼.
	•	Gu et al. (2020), “Domain-Specific Language Model Pretraining for Biomedical NLP” (PubMedBERT paper) – demonstrated PubMedBERT’s performance gains on biomedical benchmarks ￼ ￼.
	•	Huang et al. (2023), Scientific Reports – compared BioClinicalBERT vs PubMedBERT on bio NER; noted ClinicalBERT’s poor performance on literature NER ￼.
	•	HuggingFace model cards for Bio_ClinicalBERT and PubMedBERT – provided details on training data and usage ￼ ￼.
	•	Ezra Tech Blog (2021) – discussed domain vocabularies and reported PubMedBERT’s BLURB score ￼, reinforcing the domain mismatch considerations.